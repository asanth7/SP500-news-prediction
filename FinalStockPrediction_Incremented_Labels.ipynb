{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asanth7/SP500-news-prediction/blob/main/FinalStockPrediction_Incremented_Labels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sBWJXNqOcAm",
        "outputId": "712f00da-249d-4b72-cb77-59fd914312ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Dense, LSTM, Flatten, Bidirectional, Dropout\n",
        "from keras.layers.normalization.batch_normalization import BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras import regularizers\n",
        "from gc import callbacks\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import csv\n",
        "import nltk\n",
        "import string\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "STOPWORDS = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3xh8nOwOrmJ"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    if file_path is None:\n",
        "        return \"No filepath found\"\n",
        "\n",
        "    labels = []\n",
        "    headlines_per_day = []\n",
        "    with open(file_path, \"r\") as file:\n",
        "      reader = csv.reader(file, delimiter=',')\n",
        "      # next(reader, None)\n",
        "      for row in reader:\n",
        "          convertedRow = [label_or_headline for label_or_headline in row]\n",
        "          labels.append(convertedRow[0])\n",
        "          # print(convertedRow[1:])\n",
        "          headlines_per_day.append(convertedRow[1:])\n",
        "\n",
        "      file.close()\n",
        "\n",
        "      # Checks if headline is type byte from csv file, if so, decodes to string for cleaning and tokenization\n",
        "\n",
        "      for day in headlines_per_day:\n",
        "          for headline in day:\n",
        "              # print(type(headline))\n",
        "              if isinstance(headline, bytes):\n",
        "                  headline = headline.decode()\n",
        "\n",
        "      for label in labels:\n",
        "        if type(label) == int:\n",
        "          return headlines_per_day, labels\n",
        "        else:\n",
        "          int_labels = [int(label) for label in labels]\n",
        "          return headlines_per_day, int_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t3ruxFZO6HR"
      },
      "outputs": [],
      "source": [
        "file_path = r'/content/S&P500_RedditNews_labeled_data_INCREMENTED_LABELS.csv'\n",
        "headlines, labels = load_data(file_path)\n",
        "X_train, X_test, y_train, y_test = train_test_split(headlines, labels, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ0euFCAOtt6"
      },
      "outputs": [],
      "source": [
        "def clean(headline):\n",
        "    nopunct = headline.translate(str.maketrans('', '', string.punctuation))\n",
        "    nostop = [word for word in nopunct.split(' ') if word not in STOPWORDS]\n",
        "    clean_sentence = [word.lower() for word in nostop]\n",
        "    return clean_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt709UN4P29y"
      },
      "outputs": [],
      "source": [
        "X_train_token = [clean(headline) for day in X_train for headline in day]\n",
        "X_val_token = [clean(headline) for day in X_val for headline in day]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjCtNZhSP-Fg"
      },
      "outputs": [],
      "source": [
        "max_len = 0\n",
        "for phrase in X_train_token:\n",
        "    if len(phrase) > max_len:\n",
        "         max_len = len(phrase)\n",
        "for phrase in X_val_token:\n",
        "    if len(phrase) > max_len:\n",
        "        max_len = len(phrase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECrMaQ-QBdhg"
      },
      "outputs": [],
      "source": [
        "max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyxAeX9ZOxpY"
      },
      "outputs": [],
      "source": [
        "def buildVocabulary(X_train_token, X_val_token):\n",
        "\n",
        "    list_sequence = X_train_token + X_val_token\n",
        "    all_words = []\n",
        "    for phrase in list_sequence:\n",
        "      for word in phrase:\n",
        "          word = word.strip('\\n')\n",
        "          all_words.append(word)\n",
        "\n",
        "    unique_words = set(all_words)\n",
        "    print(len(unique_words))\n",
        "\n",
        "    word_count = {}\n",
        "    for word in unique_words:\n",
        "      print(word)\n",
        "      word_count[word] = all_words.count(word)\n",
        "    print('created word_count')\n",
        "\n",
        "    word2ind = {word: i for i, word in enumerate(unique_words, start=1)}\n",
        "    vocab_size = len(word2ind)\n",
        "    print('created word2ind')\n",
        "\n",
        "    return word_count, word2ind, vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab21wviOQCU3"
      },
      "outputs": [],
      "source": [
        "word_count, word2ind, vocab_size = buildVocabulary(X_train_token, X_val_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds11oBmxOztS"
      },
      "outputs": [],
      "source": [
        "def vectorize(tokens, max_len, word2ind):\n",
        "    '''\n",
        "    :param tokens:\n",
        "    :param max_len:\n",
        "    :param word2ind:\n",
        "    :return: 1D numpy array (length = max)len)\n",
        "    '''\n",
        "\n",
        "    if tokens is None:\n",
        "        return \"No tokens found\"\n",
        "    if max_len is None:\n",
        "        return \"No max_len provided\"\n",
        "    if word2ind is None:\n",
        "        return \"No word2ind found\"\n",
        "\n",
        "    sentence = np.zeros(max_len)\n",
        "    position = 0\n",
        "    for token in tokens:\n",
        "        index = word2ind.get(token, 0)\n",
        "        sentence[position] = index\n",
        "        position += 1\n",
        "\n",
        "    return sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od-_GcUwQRWF"
      },
      "outputs": [],
      "source": [
        "X_train_array = np.array([vectorize(tokens, max_len, word2ind) for tokens in X_train_token])\n",
        "X_val_array = np.array([vectorize(tokens, max_len, word2ind) for tokens in X_val_token])\n",
        "print('train and val arrays done')\n",
        "assert X_train_array.shape[-1] == max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CYOGBFYlw-D"
      },
      "outputs": [],
      "source": [
        "X_train_array.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klwNmcw0O1Va"
      },
      "outputs": [],
      "source": [
        "def convert2onehot(labels, num_classes):\n",
        "\n",
        "    if labels is None:\n",
        "        return \"No labels found\"\n",
        "\n",
        "    num_repeats = 25\n",
        "    labels = np.repeat(labels, num_repeats)\n",
        "    print(len(labels))\n",
        "\n",
        "    final = []\n",
        "    start = []\n",
        "    for i in range(num_classes):\n",
        "        start.append(0)\n",
        "\n",
        "    for label in labels:\n",
        "        values = deepcopy(start)\n",
        "        values[label] = 1\n",
        "        final.append(values)\n",
        "    return np.array(final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iJw1dYdl9-4"
      },
      "outputs": [],
      "source": [
        "y_train_onehot = convert2onehot(y_train, 10)\n",
        "y_val_onehot = convert2onehot(y_val, 10)\n",
        "print('onehots done')\n",
        "assert y_train_onehot.shape[1] == 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2riCJR0opOy5"
      },
      "outputs": [],
      "source": [
        "print(X_train_array.shape)\n",
        "print(y_train_onehot.shape)\n",
        "\n",
        "print(X_train_array)\n",
        "print('\\n')\n",
        "print(y_train_onehot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPOXjpOemCea"
      },
      "outputs": [],
      "source": [
        "# KERAS MODEL\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "stock_model = keras.Sequential([\n",
        "    Embedding(vocab_size + 1, 48, input_length=max_len),\n",
        "    LSTM(units=64, input_shape=(X_train_array.shape[0], X_train_array.shape[1]), return_sequences=True),\n",
        "    LSTM(units=32),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.35),\n",
        "    Dense(units=16, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.35),\n",
        "    Dense(units=12, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.35),\n",
        "    Dense(units=num_classes, kernel_regularizer=regularizers.l2(0.001), activation='softmax')\n",
        "])\n",
        "\n",
        "stock_model.summary()\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
        "\n",
        "# checkpoint_filepath = r'C:\\Users\\AravSanthanam\\Polygence_2022\\mdcheckpoints'\n",
        "# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#     filepath=checkpoint_filepath,\n",
        "#     save_weights_only=True,\n",
        "#     monitor='val_accuracy',\n",
        "#     mode='max',\n",
        "#     save_best_only=True)\n",
        "\n",
        "stock_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = stock_model.fit(X_train_array, y_train_onehot, batch_size=48, epochs=15, validation_data=(X_val_array, y_val_onehot), shuffle=True, callbacks=[callback], verbose=1)\n",
        "\n",
        "#stock_model.load_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfGmpHD0QSjz"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSOa1xRWlExrKw2/lUIS7I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}